{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6785994-c342-4d02-9296-6d2c992f8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aeeb7e9-6aa1-4f22-b63d-891ec65943fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 20:56:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/02 20:56:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 33986)\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"GDP_Prediction_Preprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1efd68-dc53-4bbb-9295-26cebd22dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.dirname(os.getcwd())\n",
    "base_path = os.path.join(project_root, \"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71eabbf-1b6b-483a-96ed-cb0f312d9aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset: Capital Stock from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/CapitalStockData.csv\n",
      "[INFO] Capital Stock dataset preprocessed successfully with 11640 rows and 18 columns.\n",
      "[INFO] Loading dataset: Energy Use from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/energy_use.csv\n",
      "[INFO] Energy Use dataset preprocessed successfully with 17024 rows and 3 columns.\n",
      "[INFO] Loading dataset: Labor Force from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/labor_force.csv\n",
      "[INFO] Labor Force dataset preprocessed successfully with 17024 rows and 3 columns.\n",
      "[INFO] Loading dataset: Patents from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/patents_res_nonres.csv\n",
      "[INFO] Patents dataset preprocessed successfully with 34048 rows and 3 columns.\n",
      "[INFO] Loading dataset: R&D Expenditure from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/R&D.csv\n",
      "[INFO] R&D Expenditure dataset preprocessed successfully with 17024 rows and 3 columns.\n",
      "[INFO] Loading dataset: Unemployment from /uufs/chpc.utah.edu/common/home/u1463636/CS6964/GDP_Prediction_ALITE/data/raw/unemployed_ilo_estimate.csv\n",
      "[INFO] Unemployment dataset preprocessed successfully with 17024 rows and 3 columns.\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "file_paths = {\n",
    "    \"Capital Stock\": \"CapitalStockData.csv\",\n",
    "    \"Energy Use\": \"energy_use.csv\",\n",
    "    \"Labor Force\": \"labor_force.csv\",\n",
    "    \"Patents\": \"patents_res_nonres.csv\",\n",
    "    \"R&D Expenditure\": \"R&D.csv\",\n",
    "    \"Unemployment\": \"unemployed_ilo_estimate.csv\"\n",
    "}\n",
    "\n",
    "# Load datasets into Pandas & Spark\n",
    "dfs = {}\n",
    "dfs_spark = {}\n",
    "\n",
    "for name, file in file_paths.items():\n",
    "    file_path = os.path.join(base_path, file)\n",
    "    print(f\"[INFO] Loading dataset: {name} from {file_path}\")\n",
    "    \n",
    "    # Load Pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    \n",
    "    # Rename variations of \"Country Code\"\n",
    "    rename_map = {\"country\": \"country_code\", \"countrycode\": \"country_code\", \"Country Code\": \"country_code\"}\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Identify columns that are years (e.g., \"1960_[yr1960]\")\n",
    "    year_columns = [c for c in df.columns if \"_[yr\" in c]\n",
    "\n",
    "    if year_columns:\n",
    "        # Convert wide format to long format\n",
    "        df_long = df.melt(id_vars=[\"country_code\"], value_vars=year_columns,\n",
    "                           var_name=\"year\", value_name=\"value\")\n",
    "\n",
    "        # Extract numeric year from column name (e.g., \"1960_[yr1960]\" â†’ \"1960\")\n",
    "        df_long[\"year\"] = df_long[\"year\"].str.extract(r\"(\\d{4})\").astype(int)\n",
    "        \n",
    "        # Replace original dataframe with long-format version\n",
    "        df = df_long\n",
    "\n",
    "    # Ensure the dataset has a \"year\" column\n",
    "    if \"year\" not in df.columns:\n",
    "        raise ValueError(f\"Dataset {name} does not have a 'year' column after transformation.\")\n",
    "\n",
    "    # Convert numeric columns\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in [\"country_code\", \"year\"]:\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "\n",
    "    # Fill missing values\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    # Store preprocessed Pandas DataFrame\n",
    "    dfs[name] = df\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Rename columns in Spark\n",
    "    for old_col, new_col in rename_map.items():\n",
    "        if old_col in spark_df.columns:\n",
    "            spark_df = spark_df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    # Store preprocessed Spark DataFrame\n",
    "    dfs_spark[name] = spark_df\n",
    "\n",
    "    print(f\"[INFO] {name} dataset preprocessed successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a9ace6-7d9a-48e9-86ac-0525daf24168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capital Stock': DataFrame[country_code: string, ifscode: bigint, countryname: double, year: bigint, igov_rppp: double, kgov_rppp: double, ipriv_rppp: double, kpriv_rppp: double, ippp_rppp: double, kppp_rppp: double, gdp_rppp: double, igov_n: double, kgov_n: double, ipriv_n: double, kpriv_n: double, kppp_n: double, gdp_n: double, income: double],\n",
       " 'Energy Use': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Labor Force': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Patents': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'R&D Expenditure': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Unemployment': DataFrame[country_code: string, year: bigint, value: double]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336b4bfe-3e06-436a-9864-91e44cc527e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Capital Stock': DataFrame[country_code: string, ifscode: bigint, countryname: double, year: bigint, igov_rppp: double, kgov_rppp: double, ipriv_rppp: double, kpriv_rppp: double, ippp_rppp: double, kppp_rppp: double, gdp_rppp: double, igov_n: double, kgov_n: double, ipriv_n: double, kpriv_n: double, kppp_n: double, gdp_n: double, income: double],\n",
       " 'Energy Use': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Labor Force': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Patents': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'R&D Expenditure': DataFrame[country_code: string, year: bigint, value: double],\n",
       " 'Unemployment': DataFrame[country_code: string, year: bigint, value: double]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89a36f3-ff3d-4eef-b7e7-ae88bfc4fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initial Schema of Capital Stock:\n",
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- ifscode: long (nullable = true)\n",
      " |-- countryname: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- igov_rppp: double (nullable = true)\n",
      " |-- kgov_rppp: double (nullable = true)\n",
      " |-- ipriv_rppp: double (nullable = true)\n",
      " |-- kpriv_rppp: double (nullable = true)\n",
      " |-- ippp_rppp: double (nullable = true)\n",
      " |-- kppp_rppp: double (nullable = true)\n",
      " |-- gdp_rppp: double (nullable = true)\n",
      " |-- igov_n: double (nullable = true)\n",
      " |-- kgov_n: double (nullable = true)\n",
      " |-- ipriv_n: double (nullable = true)\n",
      " |-- kpriv_n: double (nullable = true)\n",
      " |-- kppp_n: double (nullable = true)\n",
      " |-- gdp_n: double (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define output directory\n",
    "output_path = os.path.join(project_root, \"data/processed\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Load individual datasets\n",
    "df_cap = dfs_spark[\"Capital Stock\"].alias(\"cap\")\n",
    "df_energy = dfs_spark[\"Energy Use\"].alias(\"energy\")\n",
    "df_labor = dfs_spark[\"Labor Force\"].alias(\"labor\")\n",
    "df_patents = dfs_spark[\"Patents\"].alias(\"patents\")\n",
    "df_rd = dfs_spark[\"R&D Expenditure\"].alias(\"rd\")\n",
    "df_unemployment = dfs_spark[\"Unemployment\"].alias(\"unemployment\")\n",
    "\n",
    "# Print the initial schema before joining\n",
    "print(\"\\n[INFO] Initial Schema of Capital Stock:\")\n",
    "df_cap.printSchema()\n",
    "df_energy.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e86c6-a51a-4589-8bc6-f4047000b074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ebde53-7971-4aad-9746-314933548758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy = df_energy.withColumnRenamed(\"value\", \"energy_use\")\n",
    "df_labor = df_labor.withColumnRenamed(\"value\", \"labor_force\")\n",
    "df_patents = df_patents.withColumnRenamed(\"value\", \"patents\")\n",
    "df_rd = df_rd.withColumnRenamed(\"value\", \"rd_expenditure\")\n",
    "df_unemployment = df_unemployment.withColumnRenamed(\"value\", \"unemployment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9944d5a-c785-4d32-93a0-af1d0dcb30fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Joining Energy Use dataset...\n",
      "[INFO] Schema after joining Energy Use:\n",
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- ifscode: long (nullable = true)\n",
      " |-- countryname: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- igov_rppp: double (nullable = true)\n",
      " |-- kgov_rppp: double (nullable = true)\n",
      " |-- ipriv_rppp: double (nullable = true)\n",
      " |-- kpriv_rppp: double (nullable = true)\n",
      " |-- ippp_rppp: double (nullable = true)\n",
      " |-- kppp_rppp: double (nullable = true)\n",
      " |-- gdp_rppp: double (nullable = true)\n",
      " |-- igov_n: double (nullable = true)\n",
      " |-- kgov_n: double (nullable = true)\n",
      " |-- ipriv_n: double (nullable = true)\n",
      " |-- kpriv_n: double (nullable = true)\n",
      " |-- kppp_n: double (nullable = true)\n",
      " |-- gdp_n: double (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- energy_use: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] Joining Energy Use dataset...\")\n",
    "df_merged = df_cap.join(df_energy, df_cap.country_code == df_energy.country_code, \"outer\").drop(df_energy.country_code)\n",
    "print(\"[INFO] Schema after joining Energy Use:\")\n",
    "df_merged.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f45c683-aac0-41d8-bdee-0293f81436f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Joining Labor Force dataset...\n",
      "[INFO] Schema after joining Labor Force:\n",
      "\n",
      "[INFO] Joining Patents dataset...\n",
      "[INFO] Schema after joining Patents:\n",
      "\n",
      "[INFO] Joining R&D Expenditure dataset...\n",
      "[INFO] Schema after joining R&D Expenditure:\n",
      "\n",
      "[INFO] Joining Unemployment dataset...\n",
      "[INFO] Schema after joining Unemployment:\n",
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- ifscode: long (nullable = true)\n",
      " |-- countryname: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- igov_rppp: double (nullable = true)\n",
      " |-- kgov_rppp: double (nullable = true)\n",
      " |-- ipriv_rppp: double (nullable = true)\n",
      " |-- kpriv_rppp: double (nullable = true)\n",
      " |-- ippp_rppp: double (nullable = true)\n",
      " |-- kppp_rppp: double (nullable = true)\n",
      " |-- gdp_rppp: double (nullable = true)\n",
      " |-- igov_n: double (nullable = true)\n",
      " |-- kgov_n: double (nullable = true)\n",
      " |-- ipriv_n: double (nullable = true)\n",
      " |-- kpriv_n: double (nullable = true)\n",
      " |-- kppp_n: double (nullable = true)\n",
      " |-- gdp_n: double (nullable = true)\n",
      " |-- income: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- energy_use: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- labor_force: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- patents: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- rd_expenditure: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- unemployment: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join Labor Force dataset and drop duplicate `country_code`\n",
    "print(\"\\n[INFO] Joining Labor Force dataset...\")\n",
    "df_merged = df_merged.join(df_labor, df_merged.country_code == df_labor.country_code, \"outer\").drop(df_labor.country_code)\n",
    "print(\"[INFO] Schema after joining Labor Force:\")\n",
    "# df_merged.printSchema()\n",
    "\n",
    "# Join Patents dataset and drop duplicate `country_code`\n",
    "print(\"\\n[INFO] Joining Patents dataset...\")\n",
    "df_merged = df_merged.join(df_patents, df_merged.country_code == df_patents.country_code, \"outer\").drop(df_patents.country_code)\n",
    "print(\"[INFO] Schema after joining Patents:\")\n",
    "# df_merged.printSchema()\n",
    "\n",
    "# Join R&D Expenditure dataset and drop duplicate `country_code`\n",
    "print(\"\\n[INFO] Joining R&D Expenditure dataset...\")\n",
    "df_merged = df_merged.join(df_rd, df_merged.country_code == df_rd.country_code, \"outer\").drop(df_rd.country_code)\n",
    "print(\"[INFO] Schema after joining R&D Expenditure:\")\n",
    "# df_merged.printSchema()\n",
    "\n",
    "# Join Unemployment dataset and drop duplicate `country_code`\n",
    "print(\"\\n[INFO] Joining Unemployment dataset...\")\n",
    "df_merged = df_merged.join(df_unemployment, df_merged.country_code == df_unemployment.country_code, \"outer\").drop(df_unemployment.country_code)\n",
    "print(\"[INFO] Schema after joining Unemployment:\")\n",
    "df_merged.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2825dc3-1d80-4a37-843c-f55b764ccb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+----+---------+---------+----------+----------+------------------+-----------------+-----------------+------------------+-------------------+-----------------+--------------------+-----------------+----------------+------+\n",
      "|country_code|ifscode|countryname|year|igov_rppp|kgov_rppp|ipriv_rppp|kpriv_rppp|         ippp_rppp|        kppp_rppp|         gdp_rppp|            igov_n|             kgov_n|          ipriv_n|             kpriv_n|           kppp_n|           gdp_n|income|\n",
      "+------------+-------+-----------+----+---------+---------+----------+----------+------------------+-----------------+-----------------+------------------+-------------------+-----------------+--------------------+-----------------+----------------+------+\n",
      "|         AFG|    512|        NaN|1960|      3.0|     50.0|       1.0|      15.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1961|      3.0|     52.0|       1.0|      15.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1962|      4.0|     54.0|       1.0|      16.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1963|      4.0|     56.0|       1.0|      17.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1964|      4.0|     59.0|       1.0|      17.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1965|      4.0|     61.0|       2.0|      18.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1966|      4.0|     63.0|       2.0|      19.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1967|      4.0|     66.0|       2.0|      20.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1968|      5.0|     69.0|       2.0|      20.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "|         AFG|    512|        NaN|1969|      5.0|     71.0|       2.0|      21.0|0.7427164546573656|6.190049958368027|319.5912096460521|313764.81190356606|2.757517812525994E7|53660.26305876444|1.6597856587033639E7|32856.05016652789|1052414.38490676|   NaN|\n",
      "+------------+-------+-----------+----+---------+---------+----------+----------+------------------+-----------------+-----------------+------------------+-------------------+-----------------+--------------------+-----------------+----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 20:56:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/03/02 20:57:39 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 16)7]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.smj_consumeFullOuterJoinRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2201/1389917700.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/03/02 20:57:40 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 21.0 (TID 16),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.smj_consumeFullOuterJoinRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2201/1389917700.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/03/02 20:57:40 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 16) (notch308.ipoib.int.chpc.utah.edu executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.smj_consumeFullOuterJoinRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2201/1389917700.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/03/02 20:57:40 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job\n",
      "Exception in thread \"Thread-4\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o246.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 16) (notch308.ipoib.int.chpc.utah.edu executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.smj_consumeFullOuterJoinRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2201/1389917700.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:469)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.smj_consumeFullOuterJoinRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2201/1389917700.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR StatusConsoleListener An exception occurred processing Appender console\n",
      " org.apache.logging.log4j.core.appender.AppenderLoggingException: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:165)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:134)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:125)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:89)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:683)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:641)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:624)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:560)\n",
      "\tat org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:82)\n",
      "\tat org.apache.logging.log4j.core.Logger.log(Logger.java:163)\n",
      "\tat org.apache.logging.log4j.spi.AbstractLogger.tryLogMessage(AbstractLogger.java:2168)\n",
      "\tat org.apache.logging.log4j.spi.AbstractLogger.logMessageTrackRecursion(AbstractLogger.java:2122)\n",
      "\tat org.apache.logging.log4j.spi.AbstractLogger.logMessageSafely(AbstractLogger.java:2105)\n",
      "\tat org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:1980)\n",
      "\tat org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1946)\n",
      "\tat org.apache.logging.slf4j.Log4jLogger.warn(Log4jLogger.java:255)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:131)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch/local/u1463636/3429420/ipykernel_1129281/1943098083.py\", line 2, in <module>\n",
      "    df_merged.limit(10).collect()\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/sql/dataframe.py\", line 1257, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 181, in deco\n",
      "    converted = convert_exception(e.java_exception)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 157, in convert_exception\n",
      "    stacktrace: str = jvm.org.apache.spark.util.Utils.exceptionString(e)\n",
      "                      ^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1725, in __getattr__\n",
      "    raise Py4JError(message)\n",
      "py4j.protocol.Py4JError: org does not exist in the JVM\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'py4j.protocol.Py4JError'>, Py4JError('An error occurred while calling None.None'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_cap\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_merged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:157\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkUpgradeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[0;32m--> 157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     is_instance_of(gw, c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.api.python.PythonException\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n",
      "File \u001b[0;32m/uufs/chpc.utah.edu/sys/installdir/miniconda3/pyspark-3.5/lib/python3.11/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: org does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "df_cap.show(10)\n",
    "df_merged.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38c7f9-1c0b-4c57-b8c0-12c97cd91d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged dataset\n",
    "df_merged.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"\\n[INFO] Data merging complete. Output saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3963929-3d46-4ac1-a987-46204cc498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c29f7-0003-4867-a6c7-b9ff8b49bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install alite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97cf62-74ff-4b5f-aa85-221a25544ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from alite import Alite\n",
    "\n",
    "# Initialize ALITE\n",
    "print(\"\\n[INFO] Performing ALITE Integration...\")\n",
    "alite = Alite()\n",
    "\n",
    "# Add datasets to ALITE\n",
    "for name, df in dfs.items():\n",
    "    alite.add_dataframe(name, df, key=[\"country_code\", \"year\"])\n",
    "\n",
    "# Measure time for ALITE Integration\n",
    "start_time = time.time()\n",
    "alite_integrated = alite.integrate()\n",
    "alite_time = time.time() - start_time\n",
    "\n",
    "# Save ALITE merged dataset\n",
    "alite_integrated.to_csv(\"data/alite_merged.csv\", index=False)\n",
    "print(f\"[INFO] ALITE Integration Complete in {alite_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486282f3-3247-4a22-b992-16c7ae1f2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
