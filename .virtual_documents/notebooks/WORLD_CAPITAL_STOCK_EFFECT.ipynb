





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import  r2_score, root_mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import PartialDependenceDisplay


# !pip install shap


# # Load dataset
# file_path = r"C:\Users\Sushil\Desktop\STAT\WORLD_data.csv"
# df= pd.read_csv(file_path)
# df["Work_ popn"] = df["Work_popn"]*2080/1000000000 #2080 working hour per year converted to billion-h
# df = df.dropna(axis=0, how="all")
# df= df.dropna(axis=1, how="all")
# df=df.drop(["Work_popn"], axis=1)

# # Save DataFrame as a pickle file
# df.to_pickle("world_data.pkl")
# print (df.head())



# Load Dataframe from pickle file
df_memory = pd.read_pickle("world_data.pkl")
df =df_memory
print (df.head())



df=df.dropna(axis=0)
na_per_column =df.isna().sum()
total_na = df.isna().sum().sum()
print(na_per_column)
print("Total NA values:", total_na)


X = df[["G_stock", "P_stock", "Work_ popn"]]
y = df["GDP"]


X_train, X_test, y_train, y_test = train_test_split ( X, y, test_size = 0.2, random_state= 42)


# Initialize and train the Linear model
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)


coefficients = model.coef_  
intercept = model.intercept_  


print("Intercept (a):", intercept)
print("\nFeature Coefficients:")
for feature, coef in zip(X_train.columns, coefficients):
    print(f"{feature}: {coef:.4f}")

linear_equation = f"Y = {intercept:.4f}"
for feature, coef in zip(X_train.columns, coefficients):
    linear_equation += f" + ({coef:.4f} * {feature})"

print("\nLinear Regression Equation:")
print(linear_equation)


# Compute evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")


plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred, color="black", alpha=0.7)
plt.plot(y_test, y_test, color="red", linestyle="dashed", label="Perfect Prediction")
plt.xlabel("Actual GDP")
plt.ylabel("Predicted GDP")
plt.title("Actual vs. Predicted GDP")
plt.legend()
plt.show()



residuals = y_test - y_pred

plt.figure(figsize=(8,6))
sns.histplot(residuals, bins=20, kde=True, color="blue")
plt.xlabel("Residuals")
plt.title("Distribution of Residuals")
plt.show()






df_log = df.copy()
exclude_columns = ["country", "year", "population"]
transform_columns = [column for column in df.columns if column not in exclude_columns]
df_log[transform_columns] = np.log(df_log[transform_columns])
print(df_log.head())
                                   


features =["G_stock", "P_stock", "Work_ popn"]
X_log= df_log[features].values
y_log=df_log.iloc[:, -3].values

print("Class Distribution:\n", df_log.iloc[:, -3].value_counts())


# Split dataset into training 80-20
X_train, X_test, y_train, y_test = train_test_split(X_log, y_log, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



#model = Linear
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)



feature_names = list(df.drop(columns=['country', 'year','GDP', 'population']).columns)
coefficients = model.coef_  
intercept = model.intercept_  

print("Intercept (a):", intercept)
print("\nFeature Coefficients:")
for feature, coef in zip(feature_names, coefficients):
    print(f"{feature}: {coef:.4f}")

linear_equation = f"Y = {intercept:.4f}"
for feature, coef in zip(feature_names, coefficients):
    linear_equation += f" + ({coef:.4f} * {feature})"

print("\nLinear Regression Equation:")
print(linear_equation)


# Convert intercept from log-scale
A = np.exp(intercept)  

# Print log-linear form
print("\nLog-Linear Model Equation:")
log_linear_equation = f"Y = {A:.4f}"
for feature, coef in zip(feature_names, coefficients):
    log_linear_equation += f" * {feature}^{coef:.4f}"

print(log_linear_equation)



# Compute evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")


plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred, color="black", alpha=0.7)
plt.plot(y_test, y_test, color="red", linestyle="dashed", label="Perfect Prediction")
plt.xlabel("Actual GDP")
plt.ylabel("Predicted GDP")
plt.title("Actual vs. Predicted GDP")
plt.legend()
plt.show()



residuals = y_test - y_pred

plt.figure(figsize=(8,6))
sns.histplot(residuals, bins=20, kde=True, color="blue")
plt.xlabel("Residuals")
plt.title("Distribution of Residuals")
plt.show()






df_log_encoded = df_log.copy()

# One-hot encode 'country' 
df_log_encoded = pd.get_dummies(df_log_encoded, columns=['country'], drop_first=False)
# df_log_encoded = df_log_encoded.drop(columns=['country_Nepal'])  # Nepal as the reference 

# Convert year into decades 
df_log_encoded['decade'] = (df_log_encoded['year'] // 10) * 10

df_log_encoded = pd.get_dummies(df_log_encoded, columns=['decade'], drop_first= False)

# Drop original 'year' column 
df_log_encoded = df_log_encoded.drop(columns=['year'])

# Define features and target variable
X_log = df_log_encoded.drop(columns=['GDP', 'population'])  # Features
y_log = df_log_encoded['GDP']  # Target variable

# Display dataset 
print(X_log.head())



# Split 80-20
X_train, X_test, y_train, y_test = train_test_split(X_log, y_log, test_size=0.2, random_state=42)

# Standardize numerical features 
numeric_cols = ["G_stock", "P_stock", "Work_ popn"]  # Excluding one-hot encoded country columns
decade_cols = [col for col in X_log.columns if col.startswith("decade_")]
numeric_cols.extend(decade_cols)  

scaler = StandardScaler()
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])



model = LinearRegression()
model.fit(X_train, y_train)

# Predict values on test data
y_pred = model.predict(X_test)



feature_names = list(df_log_encoded.drop(columns=['GDP', 'population']).columns)  
coefficients = model.coef_
intercept = model.intercept_

print("Intercept (a):", intercept)
print("\nFeature Coefficients:")
for feature, coef in zip(feature_names, coefficients): 
    print(f"{feature}: {coef:.4f}")

# # Construct the linear regression equation
# linear_equation = f"Y = {intercept:.4f}"
# for feature, coef in zip(feature_names, coefficients):  
#     linear_equation += f" + ({coef:.4f} * {feature})"

# print("\nLinear Regression Equation:")
# print(linear_equation)



# # Compute the transformed intercept (A = e^intercept)
# A = np.exp(intercept)

# # Construct the log-linear equation
# log_linear_equation = f"Y = {A:.4f}"

# for feature, coef in zip(feature_names, coefficients):
#     if "country" in feature or "decade" in feature:
#         # Exponential transformation for categorical variables
#         log_linear_equation += f" * e^({coef:.4f} * {feature})"
#     else:
#         # Power transformation for continuous variables
#         log_linear_equation += f" * {feature}^{coef:.4f}"

# print("\nLog-Linear Model Equation:")
# print(log_linear_equation)


# Compute evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")


plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred, color="black", alpha=0.7)
plt.plot(y_test, y_test, color="red", linestyle="dashed", label="Perfect Prediction")
plt.xlabel("Actual GDP")
plt.ylabel("Predicted GDP")
plt.title("Actual vs. Predicted GDP")
plt.legend()
plt.show()


residuals = y_test - y_pred

plt.figure(figsize=(8,6))
sns.histplot(residuals, bins=20, kde=True, color="blue")
plt.xlabel("Residuals")
plt.title("Distribution of Residuals")
plt.show()






df_rf = df.copy()

log_features = ["G_stock", "P_stock", "Work_ popn", "GDP"]
for col in log_features:
    df_rf[col] = np.log(df_rf[col] + 0.001) 
    
# One-hot encode 'country'
df_rf = pd.get_dummies(df_rf, columns=['country'], drop_first=False)

# Convert 'year' into decades
df_rf['decade'] = (df_rf['year'] // 10) * 10

# Drop original 'year' column 
df_rf = df_rf.drop(columns=['year'])

# One-hot encode 'decade'
df_rf = pd.get_dummies(df_rf, columns=['decade'], drop_first=False)

# Define features and target variable
X_rf = df_rf.drop(columns=['GDP', 'population'])  # Features
y_rf = df_rf['GDP']  # Target variable 

# Display dataset 
print(X_rf.head())



# Split 80-20
X_train, X_test, y_train, y_test = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)



# Initialize and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=200,max_depth =10, random_state=42)
rf_model.fit(X_train, y_train)

# Predict values on test data
y_pred = rf_model.predict(X_test)


# Compute evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2 Score): {r2:.4f}")


plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred, color="black", alpha=0.7)
plt.plot(y_test, y_test, color="red", linestyle="dashed", label="Perfect Prediction")
plt.xlabel("Actual GDP")
plt.ylabel("Predicted GDP")
plt.title("Actual vs. Predicted GDP")
plt.legend()
plt.show()


residuals = y_test - y_pred

plt.figure(figsize=(8,6))
sns.histplot(residuals, bins=20, kde=True, color="blue")
plt.xlabel("Residuals")
plt.title("Distribution of Residuals")
plt.show()



# Extract feature importance 
feature_importances = rf_model.feature_importances_

# Normalize importance
feature_importances_normalized = feature_importances / feature_importances.sum()

# Create a DataFrame 
coefficients_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance (Approx. Coefficient)': feature_importances_normalized
}).sort_values(by='Importance (Approx. Coefficient)', ascending=False)

# Display the DataFrame 
coefficients_df['Importance (Approx. Coefficient)'] = coefficients_df['Importance (Approx. Coefficient)'].apply(lambda x: f"{x:.6f}")
print("\nRandom Forest Approximate Coefficients:")
print(coefficients_df.to_string(index=False))



feature_importances_series = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)

# Extract the top 6 features
top6_features = feature_importances_series.head(6)
print("Top 6 features by importance:")
print(top6_features)

# Plot only the top 6 features
plt.figure(figsize=(10, 6))
sns.barplot(x=top6_features.values, y=top6_features.index, palette="viridis")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Top 6 Feature Importance in Random Forest Model")
plt.show()


# Select top features for PDP visualization
top_features = coefficients_df['Feature'].head(6).tolist()

# Plot Partial Dependence Plots (PDPs) 
fig, ax = plt.subplots(figsize=(12, 8))
PartialDependenceDisplay.from_estimator(rf_model, X_train, features=top_features, ax=ax)
plt.show()



# Convert all columns in X_train to numeric 
X_train = X_train.apply(pd.to_numeric, errors='coerce')
X_test = X_test.apply(pd.to_numeric, errors='coerce')

print("Non-numeric columns:", X_train.select_dtypes(include=['O']).columns.tolist())



explainer = shap.TreeExplainer(rf_model)

# Compute SHAP values
shap_values = explainer.shap_values(X_test)

# SHAP values
shap.summary_plot(shap_values, X_test)




# SHAP dependence plot for interaction between P_stock and G_stock
shap.dependence_plot("P_stock", shap_values, X_test, interaction_index="G_stock")



# SHAP dependence plot for interaction between work_popn and P_stock
shap.dependence_plot("Work_ popn", shap_values, X_test, interaction_index= "P_stock")


# SHAP dependence plot for interaction between G_stock and work_popn
shap.dependence_plot("G_stock", shap_values, X_test, interaction_index="Work_ popn")



